\documentclass[leqno]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[]{amsthm} %lets us use \begin{proof}
\usepackage{amsmath}
\usepackage[]{amssymb} %gives us the character \varnothing
\usepackage{hyperref}
\usepackage{booktabs} % For formal tables
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{url}
%\usepackage{natbib}
\usepackage{xcolor}
\usepackage{subfig}
\let\proof\relax
\let\endproof\relax
\usepackage{enumitem}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{tablefootnote}
\usepackage{balance}
\usepackage{bibunits}

\title{Answers for Programming Part}
\author{Aline Bessa and Rao Li}
\date\today
%This information doesn't actually show up on your document unless you use the maketitle command below

\begin{document}
\maketitle %This command prints the title based on information entered above

%Section and subsection automatically number unless you put the asterisk next to them.
\section*{Programming question 1} 

\subsection*{1a} Using linear regression without regularization, we have

For the training set:
\begin{itemize}
\item TSS = 35096.8551485
\item $R^2$ = 0.750883778673
\end{itemize}

For the test set:
\begin{itemize}
\item RSS = 2479.76554682
\item TSS = 7480.04588235
\item $R^2$ = 0.668482575398
\end{itemize}

\subsection*{1b} Using ridge regression and 5-fold cross-validation to find the best $\lambda$, we have

For training set:
\begin{itemize}
\item TSS = 35096.8551485
\item $R^2$ = 0.750883778673
\end{itemize}

For test set:
\begin{itemize}
\item RSS = 2479.76554683
\item TSS = 7480.04588235
\item $R^2$ = 0.668482575397
\item $\lambda$ = 0
\end{itemize}
That is, the best $\lambda$ we found was zero, so the ridge regression coincides with the 
linear model without regularization.

\subsection*{2a} Using the model with the degree 2 polynomial transformation, without regularization, we have

For training set:
\begin{itemize}
\item TSS = 35096.8551485
\item $R^2$ = 0.936894877363
\end{itemize}

For test set:
\begin{itemize}
\item RSS = 1330.37713089
\item TSS = 7480.04588235
\item $R^2$ = 0.822143185775
\end{itemize}

\subsection*{2b} Using ridge regression coupled with the degree 2 polynomial transformation, and applying 
5-fold cross-validation to find the best $\lambda$, we have

For training set:
\begin{itemize}
\item TSS = 35096.8551485
\item $R^2$ = 0.92707931343
\end{itemize}

For test set:
\begin{itemize}
\item RSS = 1460.74019233
\item TSS = 7480.04588235
\item $R^2$ = 0.804715075909
\item $\lambda$ = 0.046
\end{itemize}

\subsection*{3} The model that performed best was the linear regression with the degree 2 
polynomial transformation because its $R^2$ is the highest over both training and test sets and its RSS is the lowest for both sets as well. The parameters are
 
\noindent \textbf{w} = [ 1.76626472e-01, -1.09328978e+01,  6.32166441e-01, -1.99036471e+00, -3.62834650e-01, -2.67607884e-01,  9.18965000e+00,  5.76236108e-01, -1.31983340e+01,  3.68757809e+00, -1.41519703e-01, -6.95003303e-01,  1.12593616e-01, -5.90040590e-01,  7.82947995e-04,  1.07238480e-01,  2.57285300e-01,  2.95831353e+00, -1.42638146e+00,  1.89522446e-01, -2.54060271e-03, -8.65280936e-02, -1.55349192e-01, -1.07956353e-03,  5.05792345e-01, -4.28887913e-04,  3.09638438e-02,  2.13294334e-04, -2.58323716e-03, -4.19134317e-02, -1.16522694e+00, -1.21535076e-02,  1.11652138e-03, -2.39047639e-02, -1.52021418e-02,  6.84239939e-04, -1.42837306e-03, -1.36803764e-05, -1.00713817e-02,  3.38488491e-02, -3.41717322e-01, -8.48792518e-02,  1.44145590e-01,  4.56318802e-03,  4.93166153e-02, -8.26983347e-02,  7.03283106e-04, -5.60402280e-02,  3.18397777e-03, -2.19000369e-02, -3.62775557e-01, -2.37519867e+00, -4.13726875e+00,  4.95118305e-02,  4.32286997e-01, -8.46039678e-01,  9.25438865e-03,  1.23472598e+00,  3.41412161e-02, -5.44958643e-01, -1.00629970e+00,  2.23053860e-01, -9.33602077e-02,  2.04329097e+01, -2.80950356e+00,  2.22805536e-01, -7.38675000e+00, -2.52743471e-02,  1.42978887e+00,  1.02702527e+00, -5.45981030e-02,  1.50093543e-02, -3.47471839e-01, -4.75685339e-03, -3.84762752e-01, -7.07761576e-03, -3.77209307e-02, -7.03466490e-05, -4.02937487e-04,  1.45757913e-02, -5.08867364e-04,  3.05327601e-03, -4.75276830e-04, -7.75137362e-03,  5.58258802e-01, -5.72302846e-02, -1.91272941e-03, -8.86877746e-02, -3.21796932e-03,  6.37305894e-02, -1.11050244e-01,  7.94070572e-03, -8.79341899e-02,  2.81185062e-03, -3.41735631e-02, -5.75953103e-05,  1.08328050e-02, -2.95897298e-04, -7.88605180e-04,  4.97528848e-02,  4.60803815e-03,  2.10932552e-02, -2.92920872e-05, -1.12958966e-04,  1.75418315e-02]

\noindent The predicted price of a house with the features in the question is 425.47739139. 

\subsection*{4} We would choose linear regression with the degree 2 polynomial transformation (no regularization) because this model 
performed best among the 4 models we attempted, as its RSS is the lowest and its $R^2$ is the highest. So it
means that our predicted function is the most likely to properly predict new housing prices, out of the tested possibilities. 

\hfill

\section*{Programming question 2}

\subsection*{1a} The answers for items in question 1a are:

\noindent (i) The predicted label is 1.

\hfill

\noindent (ii) The confusion matrix on the test set for $k = 1$ is:

\begin{tabular}{l|l|c|c|}
\multicolumn{2}{c}{}&\multicolumn{1}{c}{predicted \textit{(y)}}\\
\cline{3-4}
\multicolumn{2}{c|}{}&+&-\\
\cline{2-4}
\multirow{correct \textit{(r)}}& + & 209 & 64\\
\cline{2-4}
& - & 134 & 93 \\
\cline{2-4}
\end{tabular}

\hfill

\hfill

\noindent (iii) For $k = 1$, we have Accuracy = $\frac{TP + TN}{TP + TN + FP + FN} = \frac{209 + 93}{500} = 0.604$, true positive rate = $\frac{TP}{TP + FN} = \frac{209}{209 + 64} = 0.76556776556$, and false positive rate = $\frac{FP}{FP + TN} = \frac{134}{134 + 93} = 0.59030837004$.

\hfill

\noindent (iv) The predicted label is 1.

\hfill

\noindent (v) The confusion matrix on the test set for $k = 5$ is:

\begin{tabular}{l|l|c|c|}
\multicolumn{2}{c}{}&\multicolumn{1}{c}{predicted \textit{(y)}}\\
\cline{3-4}
\multicolumn{2}{c|}{}&+&-\\
\cline{2-4}
\multirow{correct \textit{(r)}}& + & 212 & 61\\
\cline{2-4}
& - & 136 & 91 \\
\cline{2-4}
\end{tabular}

\hfill

\hfill

\noindent (vi) For $k = 5$, we have Accuracy = $\frac{TP + TN}{TP + TN + FP + FN} = \frac{212 + 91}{500} = 0.606$, true positive rate = $\frac{TP}{TP + FN} = \frac{212}{212 + 61} = 0.77655677655$, and false positive rate = $\frac{FP}{FP + TN} = \frac{136}{136 + 91} = 0.59911894273$.

\hfill

\noindent (vii) For $k = 5$, the accuracy is 0.606, as computed above. \textbf{Am I missing something or is this question redundant given the above?}

\hfill

\noindent (viii) The confusion matrix on the test set with Zero-R is:

\begin{tabular}{l|l|c|c|}
\multicolumn{2}{c}{}&\multicolumn{1}{c}{predicted \textit{(y)}}\\
\cline{3-4}
\multicolumn{2}{c|}{}&+&-\\
\cline{2-4}
\multirow{correct \textit{(r)}}& + & 273 & 0\\
\cline{2-4}
& - & 227 & 0 \\
\cline{2-4}
\end{tabular} 

\subsection*{1b} If the documents have radically different sizes, most (or all) tokens of a small document can be in a much larger document. As a consequence, their distance is 
going to be very low, even if most tokens in the larger document do not appear in the smaller one, e.g., if the larger document focuses on a very different subject (and 
uses many  unrelated words). In other words, if the documents are out of scale, small documents are simply very likely to be close to longer documents, and the actual 
use of words in the documents (that is, their subjects) stops being relevant in the distance metric. In such cases, it is probably better to sample longer documents, or pick 
its most relevant words somehow (maybe with tf-idf), and use that when computing distances with respect to much smaller documents.



\subsection*{1c} The answers for items in question 1c are:

\noindent (i) When $k = 3$, the cross-validation accuracy is 0.66; when $k = 7$, it is 0.658; when $k = 99$, it is 0.612. The best value is then $k = 3$.

\hfill

\noindent (ii) The confusion matrix on the test set with $k = 3$ is:

\begin{tabular}{l|l|c|c|}
\multicolumn{2}{c}{}&\multicolumn{1}{c}{predicted \textit{(y)}}\\
\cline{3-4}
\multicolumn{2}{c|}{}&+&-\\
\cline{2-4}
\multirow{correct \textit{(r)}}& + & 212 & 61\\
\cline{2-4}
& - & 144 & 83 \\
\cline{2-4}
\end{tabular}  

\hfill 

\hfill

and the accuracy is $\frac{TP + TN}{TP + TN + FP + FN} = \frac{212 + 83}{500} = 0.59$.

\subsection*{1d} 

\noindent (i) The distance function we propose works in two steps: first, punctuation and the 100 most common stopwords in English are removed; then, 
the documents are represented as two vectors whose sizes correspond to the total of different tokens across them and the cosine distance is computed. Say that 
we want to compute the distance between \textit{``park slope is an incredible neighborhood''} and \textit{``the park in my neighborhood is not far.''}. After 
 removing punctuation and stopwords, we have  \textit{``park slope incredible neighborhood''} and \textit{``park neighborhood far''}. The unique tokens are
\textit{``park, slope, incredible, neighborhood, far''}, leading to word-count vectors $[1, 1, 1, 1, 0]$ and $[1, 0, 0, 1, 1]$ for the two documents 
respectively. The cosine distance between these two vectors is approximately 0.42. 

\hfill

\noindent (ii) The documents are short and many of their tokens are very common, occurring in most examples. By removing common tokens, whatever is left can 
increase the discrimination power between examples. Moreover, the cosine distance seems more robust than the metric proposed in the question, as it takes the 
number of token occurrences in a document into account, instead of simply using intersections.

\noindent (iii) The confusion matrix on the test set for $k = 1$ with our new function is:
\begin{tabular}{l|l|c|c|}
\multicolumn{2}{c}{}&\multicolumn{1}{c}{predicted \textit{(y)}}\\
\cline{3-4}
\multicolumn{2}{c|}{}&+&-\\
\cline{2-4}
\multirow{correct \textit{(r)}}& + & 204 & 69\\
\cline{2-4}
& - & 97 & 130 \\
\cline{2-4}
\end{tabular}  

\hfill 

\hfill

\noindent (iv) For $k = 1$, we have Accuracy = $\frac{TP + TN}{TP + TN + FP + FN} = \frac{204 + 130}{500} = 0.668$, true positive rate = $\frac{TP}{TP + FN} = \frac{204}{204 + 69} =  0.74725274725$, and false positive rate = $\frac{FP}{FP + TN} = \frac{97}{97 + 130} = 0.74725274725$. 

\hfill

\noindent (v) The confusion matrix on the test set for $k = 5$ with our new function is:
\begin{tabular}{l|l|c|c|}
\multicolumn{2}{c}{}&\multicolumn{1}{c}{predicted \textit{(y)}}\\
\cline{3-4}
\multicolumn{2}{c|}{}&+&-\\
\cline{2-4}
\multirow{correct \textit{(r)}}& + & 218 & 55\\
\cline{2-4}
& - & 89 & 138 \\
\cline{2-4}
\end{tabular}  

\hfill 

\hfill

\noindent (vi) For $k = 5$, we have Accuracy = $\frac{TP + TN}{TP + TN + FP + FN} = \frac{218 + 138}{500} = 0.712$, true positive rate = $\frac{TP}{TP + FN} = \frac{218}{218 + 55} = 0.79853479853$, and false positive rate = $\frac{FP}{FP + TN} = \frac{89}{89 + 138} = 0.39207048458$. 

\hfill

\noindent (vii) Our distance function achieves higher accuracy for both $k = 1$ and $k = 5$.

\end{document}
