%This is my super simple Real Analysis Homework template

\documentclass[leqno]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[]{amsthm} %lets us use \begin{proof}
\usepackage{amsmath}
\usepackage[]{amssymb} %gives us the character \varnothing

\title{Homework 2}
\author{Aline Bessa and Rao Li}
\date\today
%This information doesn't actually show up on your document unless you use the maketitle command below

\begin{document}
\maketitle %This command prints the title based on information entered above

%Section and subsection automatically number unless you put the asterisk next to them.
\section*{Question 1}

\textbf{(a)} Let $v$, $a$, and $t$ denote variables for cancer volume, age, and cancer type respectively. We then have that:
\begin{equation*}
\hat{y} = w_0 + w_1 \times v
\end{equation*}
where $w_0$ and $w_1$ are the parameters of model 1 and
\begin{equation*}
\hat{y} = w'_0 + w'_1 \times v + w'_2 \times a
\end{equation*}
where $w'_0$, $w'_1$, and $w'_2$ are the parameters of model 2.

\hfill

\noindent \textbf{(b)} Instead of using $t$, which is a categorical variable, we use one-hot encoding and generate variable $x_3$, where 1 stands for cancer type I and 
0 stands for cancer type II. We then have that:
\begin{equation*}
\hat{y} = w''_0 + x_3 \times w''_1 \times v + (1 - x_3) \times w''_2 \times v  + w''_3 \times a 
\end{equation*}
where $w''_0$, $w''_1$, $w''_2$, and $w''_3$ are the parameters of model 3.

\hfill

\noindent \textbf{(c)} Model 1 has 2 parameters and model 2 has 3 parameters. Model 2 is more complex because it has more parameters.

\hfill

\noindent \textbf{(d)} For model 1, the matrix $X'$ that corresponds to $X$'s first three rows is
\[
X'=
  \begin{bmatrix}
    1 & 0.7 \\
    1 & 1.3 \\
    1 & 1.6
  \end{bmatrix}
\]
where the values in the second column correspond to cancer volumes.

\noindent For model 2, the matrix $X'$ that corresponds to $X$'s first three rows is
\[
X'=
  \begin{bmatrix}
    1 & 0.7 & 55 \\
    1 & 1.3 & 65 \\
    1 & 1.6 & 70
  \end{bmatrix}
\]
where the second column corresponds to cancer volumes and the third column corresponds to ages.

\noindent For model 3, and considering the one-hot encoding suggested in the question, the matrix $X'$ that corresponds to $X$'s first three rows is
\[
X'=
  \begin{bmatrix}
    1 & 0.7 & 0 & 55\\
    1 & 0 & 1.3 & 65 \\
    1 & 0 & 1.6 & 70
  \end{bmatrix}
\]
where non-zero values in the second column correspond to cancer volumes for cancer type I, non-zero values in the third column correspond to cancer 
volumes for cancer type II, and the fourth column corresponds to ages.   

%% and third columns correspond to cancer volumesfirst column corresponds to cancer volumes, the second column corresponds to ages, and the third column corresponds to the one-hot encoding 
%% of the cancer types.

\hfill

\noindent \textbf{(e)} Model 3 should be selected because both its training and test MSE are the lowest.


\hfill

\section*{Question 2} Let $r$, $f$, $t$, $s$, and $y$ be the amount of rainfall, the amount of fertilizer, the temperature, the number of sunny days, and 
crop yield respectively. To formalize a polynomial, possibly linear regression problem with these variables, we make
\begin{equation*}
\hat{y} = w_0 + w_1^{k_1} \times r + w_2^{k_2} \times f + w_3^{k_3} \times t + w_4^{k_4} \times s
\end{equation*}
where $k_1$, $k_2$, $k_3$, and $k_4$ are greater than zero. If all of them are equal to 1, the regression is linear; otherwise, it's polynomial.


\section*{Question 3} \textbf{(a)} To get the final function $g(x) = w_1x + w_0$, we need to calculate coefficients $w_0$ and $w_1$. Using the closed 
formula
\begin{equation*}
\textbf{w} = (X^TX)^{-1}X^T\textbf{y}
\end{equation*}
where 
\[
X=
  \begin{bmatrix}
    1 & 1050\\
    1 & 428\\
    1 & 362\\
    1 & 529\\
    1 & 790\\
    1 & 401\\
    1 & 380\\
    1 & 1454\\
    1 & 1127\\
    1 & 700
  \end{bmatrix}
\]
and
\[
\textbf{y}=
  \begin{bmatrix}
     57\\
     28\\
     26\\
     40\\
     60\\
     22\\
     38\\
     110\\
     100\\
     46
  \end{bmatrix}
\]
we have that $w_0 = -1.97639231$ and $w_1 = 0.07571859$~\footnote{We used numpy to multiply the matrices and compute the inverse.}. Consequently,
we have that $g(x) = 0.07571859x + -1.97639231$.

\hfill

\noindent \textbf{(b)} By using this function, we get that $g(475) = 33.98993693$, a.k.a., the predicted number of stories is 34.

\hfill

\section*{Question 4} \textbf{(a)} For attribute $X_2$ we have that $x^{min} = -1$ and $x^{max} = 51$. The denominator is 
$x^{max} - x^{min} = 51 - (-1) = 52$, so
\begin{equation*}
\begin{split}
&x2^0 = \frac{42 - (-1)}{52} = \frac{43}{52} \approx 0.83 \\
&x2^1 = \frac{51 - (-1)}{52} = \frac{52}{52} = 1.00\\
&x2^2 = \frac{-1 - (-1)}{52} = 0.00\\
&x2^3 = \frac{3 - (-1)}{52} = \frac{4}{52} \approx 0.08\\
&x2^4 = \frac{26 - (-1)}{52} = \frac{27}{52} \approx 0.52\\
&x2^5 = \frac{41 - (-1)}{52} = \frac{42}{52} \approx 0.81\\ 
\end{split}
\end{equation*} 

\hfill

\noindent \textbf{(b)} In the example, the value for attribute $x1$ is 3.9; for $x2$, it is 4. The normalized example is then
\begin{equation*}
\begin{split}
&x1_{norm} = \frac{3.9 - (-0.3)}{3.8 - (-0.3)} = \frac{4.2}{4.1} \approx 1.02\\
&x2_{norm} = \frac{4 - (-1)}{52} = \frac{5}{52} \approx 0.10
\end{split}
\end{equation*} 
where -0.3 is $x^{min}$ for attribute $x1$ and 3.8 is $x^{max}$ for attribute $x1$.

\noindent To classify 
\[
x^{norm}=
  \begin{bmatrix}
     1.02\\
     0.10\\
  \end{bmatrix}
\]
we need to compute its Euclidian distance with respect to the normalized training values, that is
\begin{equation*}
\begin{split}
dist(x^{norm}, (0.68, 0.83)) = \sqrt{(1.02 - 0.68)^2 + (0.10 - 0.83)^2} \approx 0.81\\
dist(x^{norm}, (1.00, 1.00)) = \sqrt{(1.02 - 1.00)^2 + (0.10 - 1.00)^2} \approx 0.90\\
dist(x^{norm}, (0.00, 0.00)) = \sqrt{(1.02 - 0.00)^2 + (0.10 - 0.00)^2} \approx 1.02\\
dist(x^{norm}, (0.24, 0.08)) = \sqrt{(1.02 - 0.24)^2 + (0.10 - 0.08)^2} \approx 0.78\\
dist(x^{norm}, (0.46, 0.52)) = \sqrt{(1.02 - 0.46)^2 + (0.10 - 0.52)^2} \approx 0.70\\
dist(x^{norm}, (0.63, 0.81)) = \sqrt{(1.02 - 0.63)^2 + (0.10 - 0.81)^2} \approx 0.81\\ 
\end{split}
\end{equation*} 
Given that the closest point is $(0.46, 0.52)$, and given that its label is $-$, the predicted label for $x^{norm}$ is $-$.

\hfill

\section*{Question 5} \textbf{(a)} 
\begin{equation*}
\begin{split}
RSS(\textbf{w}) = \sum_{i=1}^{N}(y^{(i)} - \textbf{w}x^{(i)})^2
\end{split}
\end{equation*} 
where $N$ is the number of samples.

\hfill

\noindent \textbf{(b)} To find the \textbf{w} that minimizes RSS(\textbf{w}), we start by deriving RSS(\textbf{w}) with respect to \textbf{w}:
\begin{equation*}
\begin{split}
&\frac{dRSS(\textbf{w})}{d\textbf{w}} = \frac{d\sum_{i=1}^{N}(y^{(i)} - \textbf{w}x^{(i)})^2}{d\textbf{w}}\\
&\frac{dRSS(\textbf{w})}{d\textbf{w}} = \sum_{i=1}^{N}2(y^{(i)} - \textbf{w}x^{(i)})(-x^{(i)})\\
&\frac{dRSS(\textbf{w})}{d\textbf{w}} = -2\sum_{i=1}^{N}(y^{(i)}x^{(i)} - \textbf{w}(x^{(i)})^2)\\
&\frac{dRSS(\textbf{w})}{d\textbf{w}} = -2\sum_{i=1}^{N}y^{(i)}x^{(i)} + 2\sum_{i=1}^{N}\textbf{w}(x^{(i)})^2\\
&\frac{dRSS(\textbf{w})}{d\textbf{w}} = -2\sum_{i=1}^{N}y^{(i)}x^{(i)} + 2\textbf{w}\sum_{i=1}^{N}(x^{(i)})^2\\  
\end{split}
\end{equation*}   
Then we verify which \textbf{w} makes $\frac{dRSS(\textbf{w})}{d\textbf{w}} = 0$:
\begin{equation*}
\begin{split}
&\frac{dRSS(\textbf{w})}{d\textbf{w}} = -2\sum_{i=1}^{N}y^{(i)}x^{(i)} + 2\textbf{w}\sum_{i=1}^{N}(x^{(i)})^2\\\\
&-2\sum_{i=1}^{N}y^{(i)}x^{(i)} + 2\textbf{w}\sum_{i=1}^{N}(x^{(i)})^2 = 0\\
&2\textbf{w}\sum_{i=1}^{N}(x^{(i)})^2 = 2\sum_{i=1}^{N}y^{(i)}x^{(i)}\\
&\textbf{w} = \frac{\sum_{i=1}^{N}y^{(i)}x^{(i)}}{\sum_{i=1}^{N}(x^{(i)})^2}\\  
\end{split}
\end{equation*}   

\section*{Question 6} The error for ridge regression, $E_{ridge}(\textbf{w})$, is given by
\begin{equation*}
\begin{split}
& E_{ridge}(\textbf{w}) = E_{in}(\textbf{w}) + \lambda\textbf{w}^T\textbf{w} 
\end{split}
\end{equation*}    
where \textbf{w} is a column vector with the parameters for the regression and $\lambda$ is a non-negative 
parameter that penalizes complex models. To find the closed form solution of ridge regression, we do the following:
\begin{equation*}
\begin{split}
& \frac{dE_{ridge}(\textbf{w})}{d\textbf{w}} = \frac{d[E_{in}(\textbf{w}) + \lambda\textbf{w}^T\textbf{w}]}{d\textbf{w}}\\
& \frac{dE_{ridge}(\textbf{w})}{d\textbf{w}} = \frac{d[E_{in}(\textbf{w}) + \textbf{w}^T\lambda\textbf{I}\textbf{w}]}{d\textbf{w}}\\
& \frac{dE_{ridge}(\textbf{w})}{d\textbf{w}} = \frac{dE_{in}(\textbf{w})}{d\textbf{w}} + \frac{d\textbf{w}^T\lambda\textbf{I}\textbf{w}}{d\textbf{w}}\\
& \frac{dE_{ridge}(\textbf{w})}{d\textbf{w}} = \frac{d}{d\textbf{w}}[\frac{1}{N}||X\textbf{w} - y||^2_2] + \frac{d\textbf{w}^T\lambda\textbf{I}\textbf{w}}{d\textbf{w}}\\
\end{split}
\end{equation*}
where \textbf{I} is the identity, important to guarantee that the $\lambda$ term is compatible, dimension-wise, with the derivative of $E_{in}$. As seen in class, 
we then have that
\begin{equation*}
\begin{split}
& \frac{dE_{ridge}(\textbf{w})}{d\textbf{w}} = \frac{2}{N}(X^TX\textbf{w} - X^Ty) + \frac{d\textbf{w}^T\lambda\textbf{I}\textbf{w}}{d\textbf{w}}\\
\end{split}
\end{equation*}
Finally, we have that
\begin{equation*}
\begin{split}
& \frac{dE_{ridge}(\textbf{w})}{d\textbf{w}} = \frac{2}{N}(X^TX\textbf{w} - X^Ty) + 2\lambda\textbf{I}\textbf{w}\\
\end{split}
\end{equation*}
To get the parameters \textbf{w} that minimize $E_{ridge}$, we set the equation above to zero:
\begin{equation*}
\begin{split}
& \frac{2}{N}(X^TX\textbf{w} - X^Ty) + 2\lambda\textbf{I}\textbf{w} = 0\\
& \frac{2}{N}X^TX\textbf{w} - \frac{2}{N}X^Ty + 2\lambda\textbf{I}\textbf{w} = 0\\
& \frac{X^TX\textbf{w}}{N} - \frac{X^Ty}{N} + \lambda\textbf{I}\textbf{w} = 0\\
& X^TX\textbf{w} - X^Ty + \lambda N\textbf{I}\textbf{w} = 0\\
& X^TX\textbf{w} + \lambda N\textbf{I}\textbf{w} = X^Ty\\
& (X^TX + \lambda N\textbf{I})\textbf{w} = X^Ty\\
& \textbf{w} = (X^TX + \lambda N\textbf{I})^{-1}X^Ty
\end{split}
\end{equation*}

\hfill

\section*{Question 7} \textbf{(a)} The true positive rate for Table 1 is
\begin{equation*}
\begin{split}
TPR = \frac{0}{3} = 0
\end{split}
\end{equation*} 
where TPR stands for \textit{True Positive Rate}.

\hfill

\noindent \textbf{(b)} The false positive rate for Table 2 is
\begin{equation*}
\begin{split}
FPR = \frac{1}{3} \approx 0.33
\end{split}
\end{equation*} 
where FPR stands for \textit{False Positive Rate}.

\hfill

\noindent \textbf{(c)} The accuracy ($A$) is given by
\begin{equation*}
\begin{split}
A = \frac{TP + TN}{P + N}
\end{split}
\end{equation*} 
where TP is the number of true positives, TN is the number of true negatives, P is the total of positives, and N is 
the total of negatives. The accuracy for Table 4 then is
\begin{equation*}
\begin{split}
A = \frac{56 + 41}{58 + 42} = \frac{97}{100} = 0.97 
\end{split}
\end{equation*} 

\section*{Question 8} \noindent \textbf{(a)} According to the data in the question, \textit{spam} is the positive class (+) and \textit{non-spam} is the negative class. 
Given that a false negative prediction costs 5 units, and that a false positive prediction costs 2, we have that
\begin{equation*}
\begin{split}
&E[predicting\mbox{ }spam] = 2 \times P(non-spam|x1, x2)\\
&E[predicting\mbox{ }spam] = 2 \times (1 - P(spam|x1, x2))\\
&E[predicting\mbox{ }spam] = 2 \times (1 - \frac{1}{1 + e^{-(3x_2 - 2x_1 + 1)}})\\
&E[predicting\mbox{ }spam] = 2 \times (1 - \frac{1}{1 + e^{-(3 \times 2 - 2 \times 3 + 1)}})\\
&E[predicting\mbox{ }spam] = 2 \times (1 - \frac{1}{1 + e})\\
&E[predicting\mbox{ }spam] \approx 2 \times (1 - 0.26894142137)\\
&E[predicting\mbox{ }spam] \approx 2 \times 0.73105857863\\
&E[predicting\mbox{ }spam] \approx 1.46211715726
\end{split}
\end{equation*} 
and
\begin{equation*}
\begin{split}
&E[predicting\mbox{ }non-spam] = 5 \times P(spam|x1, x2)\\
&E[predicting\mbox{ }non-spam] = 5 \times \frac{1}{1 + e^{-(3x_2 - 2x_1 + 1)}}\\
&E[predicting\mbox{ }non-spam] = 5 \times \frac{1}{1 + e^{-(3 \times 2 - 2 \times 3 + 1)}}\\
&E[predicting\mbox{ }non-spam] = 5 \times \frac{1}{1 + e}\\
&E[predicting\mbox{ }non-spam] \approx 5 \times 0.26894142137\\
&E[predicting\mbox{ }non-spam] \approx 1.34470710685
\end{split}
\end{equation*} 
That is, classifying as \textit{non-spam} has smaller expected risk.

\section*{Question 9} Let the estimator of the mean of $D$ be
\begin{equation*}
\begin{split}
&\hat{\theta} = \frac{\sum_{x \in X}x}{N + 1}
\end{split}
\end{equation*} 
Moreover, let $\theta = \mu$ be the actual mean of the Gaussian distribution $D$. The bias of $\hat{\theta}$ is 
\begin{equation*}
\begin{split}
&Bias_{\theta}[\hat{\theta}] =  E[\hat{\theta}] - \mu\\
&Bias_{\theta}[\hat{\theta}] =  E[\frac{\sum_{x \in X}x}{N + 1}] - \mu\\
&Bias_{\theta}[\hat{\theta}] = \frac{E[\sum_{x \in X}x]}{N + 1} - \mu\\
&Bias_{\theta}[\hat{\theta}] = \frac{E[x_1] + \ldots + E[x_N]}{N + 1} - \mu\\
\end{split}
\end{equation*}  
Given that $D$ is Gaussian, $E[x_i] = \mu$. Consequently,
\begin{equation*}
\begin{split}
&Bias_{\theta}[\hat{\theta}] = \frac{\mu + \ldots + \mu}{N + 1} - \mu\\
&Bias_{\theta}[\hat{\theta}] = \frac{N\mu}{N + 1} - \mu\\
&Bias_{\theta}[\hat{\theta}] = \frac{N\mu - (N + 1)\mu}{N + 1}\\
&Bias_{\theta}[\hat{\theta}] = \frac{-\mu}{N + 1}\\
\end{split}
\end{equation*}  
The estimator $\hat{\theta}$ is thus biased, as long as the numerator of $Bias_{\theta}[\hat{\theta}]$ is different from zero.

\hfill

\section*{Question 10}  \textbf{Important note: in this question, we work with two decimal places for simplicity, as a specific precision is not specified in the question. 
Results would change if we had used a different precision}. 

\noindent \textbf{(a)} Given the dataset, we have the following positive examples:
\begin{gather*}
\begin{split}
&\textbf{x}_+^1 = 
 \begin{bmatrix}
   2.7 \\
   4.8 \\
 \end{bmatrix}\\
&\textbf{x}_+^2 = 
 \begin{bmatrix}
   3.2 \\
   5.1 \\
\end{bmatrix}\\
&\textbf{x}_+^3 = 
 \begin{bmatrix}
   -0.4 \\
   -0.3\\
 \end{bmatrix}
\end{split}
\end{gather*}
where the first and second rows correspond, respectively, to values for attributes $x1$ and $x2$. 
Consequently,  
\begin{gather*}
\begin{split}
&\hat{\mu}_+ = 
 \frac{ \begin{bmatrix}
   2.7 \\
   4.8\\ 
 \end{bmatrix} +  \begin{bmatrix}
   3.2 \\
   5.1\\ 
 \end{bmatrix} +  \begin{bmatrix}
   -0.4 \\
   -0.3 \\
 \end{bmatrix}}{3}\\
&\hat{\mu}_+ = 
 \frac{\begin{bmatrix}
   2.7 + 3.2 - 0.4\\
   4.8 + 5.1 - 0.3\\ 
   \end{bmatrix}}{3}\\
&\hat{\mu}_+ = 
 \frac{\begin{bmatrix}
   5.5\\
   9.6 
   \end{bmatrix}}{3}\\
&\hat{\mu}_+ \approx 
\begin{bmatrix}
   1.83\\
   3.20 
   \end{bmatrix}
\end{split}
\end{gather*}
where $\hat{\mu}_+[1][1] \approx 1.83$ and $\hat{\mu}_+[2][1] \approx 3.20$ are the estimated means for attributes $x1$ and $x2$ respectively, when labels are positive.

\noindent Analogously, the negative examples are
\begin{gather*}
\begin{split}
&\textbf{x}_-^1 = 
 \begin{bmatrix}
   0.6 \\
   0.5 \\
 \end{bmatrix}\\
&\textbf{x}_-^2 = 
 \begin{bmatrix}
   1.8 \\
   2.8 \\
\end{bmatrix}\\
&\textbf{x}_-^3 = 
 \begin{bmatrix}
   2.1 \\
   4.3\\
 \end{bmatrix}
\end{split}
\end{gather*}
Consequently,
\begin{gather*}
\begin{split}
&\hat{\mu}_- = 
 \frac{ \begin{bmatrix}
   0.6 \\
   0.5\\ 
 \end{bmatrix} +  \begin{bmatrix}
   1.8 \\
   2.8\\ 
 \end{bmatrix} +  \begin{bmatrix}
   2.1\\
   4.3\\
 \end{bmatrix}}{3}\\
&\hat{\mu}_- = 
 \frac{\begin{bmatrix}
   0.6 + 1.8 + 2.1\\
   0.5 + 2.8 + 4.3\\ 
   \end{bmatrix}}{3}\\
&\hat{\mu}_- = 
 \frac{\begin{bmatrix}
   4.5\\
   7.6 
   \end{bmatrix}}{3}\\
&\hat{\mu}_- \approx 
\begin{bmatrix}
   1.50\\
   2.53 
   \end{bmatrix}
\end{split}
\end{gather*}
where $\hat{\mu}_+[1][1] \approx 1.50$ and $\hat{\mu}_+[2][1] \approx 2.53$ are the estimated means for attributes $x1$ and $x2$ respectively, when labels are negative.

\noindent Given the positive examples and $\hat{\mu}_+$, we compute
\begin{gather*}
\begin{split}
&\textbf{x}_+^1 - \hat{\mu}_+ = 
\begin{bmatrix}
   2.7\\
   4.8 
\end{bmatrix} - 
\begin{bmatrix}
   1.83\\
   3.20 
\end{bmatrix} = 
\begin{bmatrix}
   0.87\\
   1.60 
\end{bmatrix}\\
&\textbf{x}_+^2 - \hat{\mu}_+ = 
\begin{bmatrix}
   3.2\\
   5.1 
\end{bmatrix} - 
\begin{bmatrix}
   1.83\\
   3.20 
\end{bmatrix} = 
\begin{bmatrix}
   1.37\\
   1.90 
\end{bmatrix}\\
&\textbf{x}_+^3 - \hat{\mu}_+ = 
\begin{bmatrix}
   -0.4\\
   -0.3 
\end{bmatrix} - 
\begin{bmatrix}
   1.83\\
   3.20 
\end{bmatrix} = 
\begin{bmatrix}
   -2.23\\
   -3.50 
\end{bmatrix}\\
\end{split}
\end{gather*}
Thus
\begin{gather*}
\begin{split}
&\hat{\Sigma}_+ = 
\frac{\begin{bmatrix}
   0.87\\
   1.60 
\end{bmatrix} \times \begin{bmatrix}
   0.87 & 1.60\\
\end{bmatrix} + 
\begin{bmatrix}
   1.37\\
   1.90 
\end{bmatrix} \times \begin{bmatrix}
   1.37 & 1.90\\
\end{bmatrix} +
\begin{bmatrix}
   -2.23\\
   -3.50 
\end{bmatrix} \times \begin{bmatrix}
   -2.23 & -3.50\\
\end{bmatrix}}{3}\\ 
&\hat{\Sigma}_+ = 
\frac{\begin{bmatrix}
   0.76 & 1.39\\
   1.39 & 2.56 
\end{bmatrix} + 
\begin{bmatrix}
   1.88 & 2.60\\
   2.60 & 3.61 
\end{bmatrix} +
\begin{bmatrix}
   4.97 & 7.81\\
   7.81 & 12.25 
\end{bmatrix}
 }{3}\\
&\hat{\Sigma}_+ = 
\frac{\begin{bmatrix}
   7.61 & 11.8\\
   11.8 & 18.42 
\end{bmatrix}}{3}\\
&\hat{\Sigma}_+ = 
\begin{bmatrix}
   2.54 & 3.93\\
   3.93 & 6.14 
\end{bmatrix}\\
\end{split}
\end{gather*}

\noindent Analogously, given the negative examples and $\hat{\mu}_-$, we compute
\begin{gather*}
\begin{split}
&\textbf{x}_-^1 - \hat{\mu}_- = 
\begin{bmatrix}
   0.6\\
   0.5 
\end{bmatrix} - 
\begin{bmatrix}
   1.50\\
   2.53 
\end{bmatrix} = 
\begin{bmatrix}
   -0.90\\
   -2.03 
\end{bmatrix}\\
&\textbf{x}_-^2 - \hat{\mu}_- = 
\begin{bmatrix}
   1.8\\
   2.8 
\end{bmatrix} - 
\begin{bmatrix}
   1.50\\
   2.53 
\end{bmatrix} = 
\begin{bmatrix}
   0.30\\
   0.27 
\end{bmatrix}\\
&\textbf{x}_-^3 - \hat{\mu}_- = 
\begin{bmatrix}
   2.1\\
   4.3 
\end{bmatrix} - 
\begin{bmatrix}
   1.50\\
   2.53 
\end{bmatrix} = 
\begin{bmatrix}
   0.60\\
   1.77 
\end{bmatrix}\\
\end{split}
\end{gather*}
Thus
\begin{gather*}
\begin{split}
&\hat{\Sigma}_- = 
\frac{\begin{bmatrix}
   -0.90\\
   -2.03 
\end{bmatrix} \times \begin{bmatrix}
   -0.90 & -2.03\\
\end{bmatrix} + 
\begin{bmatrix}
   0.30\\
   0.27 
\end{bmatrix} \times \begin{bmatrix}
   0.30 & 0.27\\
\end{bmatrix} +
\begin{bmatrix}
   0.60\\
   1.77 
\end{bmatrix} \times \begin{bmatrix}
   0.60 & 1.77\\
\end{bmatrix}}{3}\\ 
&\hat{\Sigma}_- = 
\frac{\begin{bmatrix}
   0.81 & 1.83\\
   1.83 & 4.12 
\end{bmatrix} + 
\begin{bmatrix}
   0.09 & 0.08\\
   0.08 & 0.07 
\end{bmatrix} +
\begin{bmatrix}
   0.36 & 1.06\\
   1.06 & 3.13 
\end{bmatrix}
 }{3}\\
&\hat{\Sigma}_- = 
\frac{\begin{bmatrix}
   1.26 & 2.97\\
   2.97 & 7.33 
\end{bmatrix}}{3}\\
&\hat{\Sigma}_- = 
\begin{bmatrix}
   0.42 & 0.99\\
   0.99 & 2.44 
\end{bmatrix}\\
\end{split}
\end{gather*}

\hfill

\noindent \textbf{(b)} Given that $d = 2$, the pdf is
\begin{equation*}
\begin{split}
&p(\textbf{x}) = \frac{1}{2\pi|\Sigma|^{1/2}}e^{-\frac{1}{2}(\textbf{x} - \mu)^T\Sigma^{-1}(\textbf{x} - \mu)}\\
&\log p(\textbf{x}) = \frac{-\frac{1}{2}(\textbf{x} - \mu)^T\Sigma^{-1}(\textbf{x} - \mu)}{2\pi|\Sigma|^{1/2}} 
\end{split}
\end{equation*}
We then use $\hat{\mu}_+$ and $\hat{\Sigma}_+$ to compute $\log p(\textbf{x}|+)$:
\begin{gather*}
\begin{split}
&\log p(\textbf{x}|+) = 
\frac{-\frac{1}{2}(\begin{bmatrix}
   1.6\\
   2.3 
\end{bmatrix} - \begin{bmatrix}
   1.83\\
   3.20 
\end{bmatrix})^T\begin{bmatrix}
   2.54 & 3.93\\
   3.93 & 6.14 
\end{bmatrix}^{-1}(\begin{bmatrix}
   1.6\\
   2.3 
\end{bmatrix} - \begin{bmatrix}
   1.83\\
   3.20 
\end{bmatrix})}{2\pi\det{\begin{bmatrix}
   2.54 & 3.93\\
   3.93 & 6.14 
\end{bmatrix}}^{1/2}}\\
&\log p(\textbf{x}|+) = 
\frac{-\frac{1}{2}\begin{bmatrix}
   -0.23\\
   -0.90 
\end{bmatrix}^T\begin{bmatrix}
   2.54 & 3.93\\
   3.93 & 6.14 
\end{bmatrix}^{-1}\begin{bmatrix}
   -0.23\\
   -0.90 
\end{bmatrix}}{2\pi\det{\begin{bmatrix}
   2.54 & 3.93\\
   3.93 & 6.14 
\end{bmatrix}}^{1/2}}\\
&\log p(\textbf{x}|+) = 
\frac{-\frac{1}{2}\begin{bmatrix}
   -0.23 & -0.90 
\end{bmatrix}\begin{bmatrix}
   40.74 & -26.08\\
   -26.08 & 16.85 
\end{bmatrix}\begin{bmatrix}
   -0.23\\
   -0.90 
\end{bmatrix}}{2\pi\det{\begin{bmatrix}
   2.54 & 3.93\\
   3.93 & 6.14 
\end{bmatrix}}^{1/2}}\\
&\log p(\textbf{x}|+) = 
\frac{-\frac{1}{2}\begin{bmatrix}
   14.10 & -9.17\\ 
\end{bmatrix}\begin{bmatrix}
   -0.23\\
   -0.90 
\end{bmatrix}}{2\pi\det{\begin{bmatrix}
   2.54 & 3.93\\
   3.93 & 6.14 
\end{bmatrix}}^{1/2}}\\
&\log p(\textbf{x}|+) = 
\frac{-\frac{1}{2}\begin{bmatrix}
   5.01\\ 
\end{bmatrix}}{2\pi\det{\begin{bmatrix}
   2.54 & 3.93\\
   3.93 & 6.14 
\end{bmatrix}}^{1/2}}\\
&\log p(\textbf{x}|+) = 
\frac{-\frac{1}{2}\begin{bmatrix}
   5.01\\ 
\end{bmatrix}}{2\pi \times \sqrt{0.15}}\\
&\log p(\textbf{x}|+) = 
\frac{-2.51}{2.44}  \approx -1.03
\end{split}
\end{gather*}

As for $\log p(\textbf{x}|-)$, we use $\hat{\mu}_-$ and $\hat{\Sigma}_-$:
\begin{gather*}
\begin{split}
&\log p(\textbf{x}|-) = 
\frac{-\frac{1}{2}(\begin{bmatrix}
   1.6\\
   2.3 
\end{bmatrix} - \begin{bmatrix}
   1.50\\
   2.53 
\end{bmatrix})^T\begin{bmatrix}
   0.42 & 0.99\\
   0.99 & 2.44 
\end{bmatrix}^{-1}(\begin{bmatrix}
   1.6\\
   2.3 
\end{bmatrix} - \begin{bmatrix}
   1.50\\
   2.53
\end{bmatrix})}{2\pi\det{\begin{bmatrix}
   0.42 & 0.99\\
   0.99 & 2.44 
\end{bmatrix}}^{1/2}}\\
&\log p(\textbf{x}|-) = 
\frac{-\frac{1}{2}\begin{bmatrix}
   0.10\\
  -0.23 
\end{bmatrix}^T\begin{bmatrix}
   0.42 & 0.99\\
   0.99 & 2.44 
\end{bmatrix}^{-1}\begin{bmatrix}
   0.10\\
  -0.23 
\end{bmatrix}}{2\pi\det{\begin{bmatrix}
   0.42 & 0.99\\
   0.99 & 2.44 
\end{bmatrix}}^{1/2}}\\
&\log p(\textbf{x}|-) = 
\frac{-\frac{1}{2}\begin{bmatrix}
   0.10 & -0.23 
\end{bmatrix}\begin{bmatrix}
   54.59 & -22.15\\
   -22.15 & 9.40 
\end{bmatrix}\begin{bmatrix}
   0.10\\
   -0.23 
\end{bmatrix}}{2\pi\det{\begin{bmatrix}
   0.42 & 0.99\\
   0.99 & 2.44 
\end{bmatrix}}^{1/2}}\\
&\log p(\textbf{x}|-) = 
\frac{-\frac{1}{2}\begin{bmatrix}
   10.55 & -4.38\\ 
\end{bmatrix}\begin{bmatrix}
   0.10\\
  -0.23 
\end{bmatrix}}{2\pi\det{\begin{bmatrix}
   0.42 & 0.99\\
   0.99 & 2.44 
\end{bmatrix}}^{1/2}}\\
&\log p(\textbf{x}|-) = 
\frac{-\frac{1}{2}\begin{bmatrix}
   2.06\\ 
\end{bmatrix}}{2\pi\det{\begin{bmatrix}
   0.42 & 0.99\\
   0.99 & 2.44 
\end{bmatrix}}^{1/2}}\\
&\log p(\textbf{x}|-) = 
\frac{-\frac{1}{2}\begin{bmatrix}
   2.06\\ 
\end{bmatrix}}{2\pi \times \sqrt{0.04}}\\
&\log p(\textbf{x}|-) = 
\frac{-1.03}{1.33}\\
& \log p(\textbf{x}|-) \approx -0.78
\end{split}
\end{gather*}
Using the ML hypothesis, \textbf{x} is predicted as $-$ because $\log p(\textbf{x}|-) > \log p(\textbf{x}|+)$. %\textbf{check calculations}

\hfill

\noindent \textbf{(c)} A good reason to use a single covariance matrix is that it requires the estimation of fewer parameters (if a lot of 
data is not available, this is particularly useful). On the other hand, if the values for different 
classes vary in significantly different ways, this information gets lost when one uses a single covariance matrix, and may  affect predictions negatively. 
If there is a lot of data available for all classes, it is probably a good idea to use different covariance matrices; otherwise, it is probably more useful 
to have a shared matrix.


\end{document}

