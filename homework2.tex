%This is my super simple Real Analysis Homework template

\documentclass[leqno]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[]{amsthm} %lets us use \begin{proof}
\usepackage{amsmath}
\usepackage[]{amssymb} %gives us the character \varnothing

\title{Homework 2}
\author{Aline Bessa and Li Rao}
\date\today
%This information doesn't actually show up on your document unless you use the maketitle command below

\begin{document}
\maketitle %This command prints the title based on information entered above

%Section and subsection automatically number unless you put the asterisk next to them.
\section*{Question 1}

\textbf{(a)} Let $v$, $a$, and $t$ denote variables for cancer volume, age, and cancer type respectively. We then have that:
\begin{equation*}
\hat{y} = w_0 + w_1 \times v
\end{equation*}
where $w_0$ and $w_1$ are the parameters of model 1 and
\begin{equation*}
\hat{y} = w'_0 + w'_1 \times v + w'_2 \times a
\end{equation*}
where $w'_0$, $w'_1$, and $w'_2$ are the parameters of model 2.

\hfill

\noindent \textbf{(b)} Instead of using $t$, which is a categorical variable, we use one-hot encoding and generate variable $x_3$, where 1 stands for cancer type I and 
0 stands for cancer type II. We then have that:
\begin{equation*}
\hat{y} = w''_0 + x_3 \times w''_1 \times v + (1 - x_3) \times w''_2 \times v  + w''_3 \times a 
\end{equation*}
where $w''_0$, $w''_1$, $w''_2$, and $w''_3$ are the parameters of model 3.

\hfill

\noindent \textbf{(c)} Model 1 has 2 parameters and model 2 has 3 parameters. Model 2 is more complex because it has more parameters.

\hfill

\noindent \textbf{(d)} For model 1, the matrix $X'$ that corresponds to $X$'s first three rows is
\[
X'=
  \begin{bmatrix}
    1 & 0.7 \\
    1 & 1.3 \\
    1 & 1.6
  \end{bmatrix}
\]
where the values correspond to cancer volumes.

\noindent For model 2, the matrix $X'$ that corresponds to $X$'s first three rows is
\[
X'=
  \begin{bmatrix}
    1 & 0.7 & 55 \\
    1 & 1.3 & 65 \\
    1 & 1.6 & 70
  \end{bmatrix}
\]
where the first column corresponds to cancer volumes and the second column corresponds to ages.

\noindent For model 3, and considering the one-hot encoding suggested in the question, the matrix $X'$ that corresponds to $X$'s first three rows is
\[
X'=
  \begin{bmatrix}
    1 & 0.7 & 0 & 55\\
    1 & 0 & 1.3 & 65 \\
    1 & 0 & 1.6 & 70
  \end{bmatrix}
\]
where the first column corresponds to cancer volumes, the second column corresponds to ages, and the third column corresponds to the one-hot encoding 
of the cancer types.

\hfill

\noindent \textbf{(e)} Model 3 should be selected because both its training and test MSE are the lowest.


\hfill

\section*{Question 2} Let $r$, $f$, $t$, $s$, and $y$ be the amount of rainfall, the amount of fertilizer, the temperature, the number of sunny days, and 
crop yield respectively. Then we have that
\begin{equation*}
\hat{y} = w_0 + w_1^{k_1} \times r + w_2^{k_2} \times f + w_3^{k_3} \times t + w_4^{k_4} \times s
\end{equation*}
where $k_1$, $k_2$, $k_3$, and $k_4$ are greater than zero. If all of them are equal to 1, the regression is linear; otherwise, it's polynomial.
\textbf{check with Linda}

\section*{Question 3} \textbf{(a)} To get the final function $g(x) = w_1x + w_0$, we need to calculate coefficients $w_0$ and $w_1$. Using the closed 
formula
\begin{equation*}
\textbf{w} = (X^TX)^-1X^T\textbf{y}
\end{equation*}
where 
\[
X=
  \begin{bmatrix}
    1 & 1050\\
    1 & 428\\
    1 & 362\\
    1 & 529\\
    1 & 790\\
    1 & 401\\
    1 & 380\\
    1 & 1454\\
    1 & 1127\\
    1 & 700
  \end{bmatrix}
\]
and
\[
\textbf{y}=
  \begin{bmatrix}
     57\\
     28\\
     26\\
     40\\
     60\\
     22\\
     38\\
     110\\
     100\\
     46
  \end{bmatrix}
\]
we have that $w_0 = -1.97639231$ and $w_1 = 0.07571859$~\footnote{We used numpy to multiply the matrices and compute the inverse.}. Consequently,
we have that $g(x) = 0.07571859x + -1.97639231$.

\hfill

\noindent \textbf{(b)} By using this function, we get that $g(475) = 33.98993693$, a.k.a., the predicted number of stories is 34.

\hfill

\section*{Question 4} \textbf{(a)} For attribute $X_2$ we have that $x^{min} = 1$ and $x^{max} = 51$. The denominator is 
$x^{max} - x^{min} = 51 - (-1) = 52$, so
\begin{equation*}
\begin{split}
&x2^0 = \frac{42 - (-1)}{52} = \frac{43}{52} \approx 0.83 \\
&x2^1 = \frac{51 - (-1)}{52} = \frac{52}{52} = 1.00\\
&x2^2 = \frac{-1 - (-1)}{52} = 0.00\\
&x2^3 = \frac{3 - (-1)}{52} = \frac{4}{52} \approx 0.08\\
&x2^4 = \frac{26 - (-1)}{52} = \frac{27}{52} \approx 0.52\\
&x2^5 = \frac{41 - (-1)}{52} = \frac{42}{52} \approx 0.81\\ 
\end{split}
\end{equation*} 

\hfill

\noindent \textbf{(b)} In the example, the value for attribute $x1$ is 3.9; for $x2$, it is 4. The normalized example is then
\begin{equation*}
\begin{split}
&x1_{norm} = \frac{3.9 - (-0.3)}{3.8 - (-0.3)} = \frac{4.2}{4.1} \approx 1.02\\
&x2_{norm} = \frac{4 - (-1)}{52} = \frac{5}{52} \approx 0.10
\end{split}
\end{equation*} 
where -0.3 is $x^{min}$ for attribute $x1$ and 3.8 is $x^{max}$ for attribute $x1$.

\hfill

\section*{Question 5} \textbf{(a)} 
\begin{equation*}
\begin{split}
RSS(\textbf{w}) = \sum_{i=1}^{N}(y^{(i)} - \textbf{w}x^{(i)})^2
\end{split}
\end{equation*} 
where $N$ is the number of samples.

\hfill

\noindent \textbf{(b)} To find the \textbf{w} that minimizes RSS(\textbf{w}), we start by deriving RSS(\textbf{w}) with respect to \textbf{w}:
\begin{equation*}
\begin{split}
&\frac{dRSS(\textbf{w})}{d\textbf{w}} = \frac{d\sum_{i=1}^{N}(y^{(i)} - \textbf{w}x^{(i)})^2}{d\textbf{w}}\\
&\frac{dRSS(\textbf{w})}{d\textbf{w}} = \sum_{i=1}^{N}2(y^{(i)} - \textbf{w}x^{(i)})(-x^{(i)})\\
&\frac{dRSS(\textbf{w})}{d\textbf{w}} = -2\sum_{i=1}^{N}(y^{(i)}x^{(i)} - \textbf{w}(x^{(i)})^2)\\
&\frac{dRSS(\textbf{w})}{d\textbf{w}} = -2\sum_{i=1}^{N}y^{(i)}x^{(i)} + 2\sum_{i=1}^{N}\textbf{w}(x^{(i)})^2\\
&\frac{dRSS(\textbf{w})}{d\textbf{w}} = -2\sum_{i=1}^{N}y^{(i)}x^{(i)} + 2\textbf{w}\sum_{i=1}^{N}(x^{(i)})^2\\  
\end{split}
\end{equation*}   
Then we verify which \textbf{w} makes $\frac{dRSS(\textbf{w})}{d\textbf{w}} = 0$:
\begin{equation*}
\begin{split}
&\frac{dRSS(\textbf{w})}{d\textbf{w}} = -2\sum_{i=1}^{N}y^{(i)}x^{(i)} + 2\textbf{w}\sum_{i=1}^{N}(x^{(i)})^2\\\\
&-2\sum_{i=1}^{N}y^{(i)}x^{(i)} + 2\textbf{w}\sum_{i=1}^{N}(x^{(i)})^2 = 0\\
&2\textbf{w}\sum_{i=1}^{N}(x^{(i)})^2 = 2\sum_{i=1}^{N}y^{(i)}x^{(i)}\\
&\textbf{w} = \frac{\sum_{i=1}^{N}y^{(i)}x^{(i)}}{\sum_{i=1}^{N}(x^{(i)})^2}\\  
\end{split}
\end{equation*}   

\section*{Question 6}

\hfill

\section*{Question 7} \textbf{(a)} The true positive rate for Table 1 is
\begin{equation*}
\begin{split}
TPR = \frac{0}{3} = 0
\end{split}
\end{equation*} 
where TPR stands for \textit{True Positive Rate}.

\hfill

\noindent \textbf{(b)} The false positive rate for Table 2 is
\begin{equation*}
\begin{split}
FPR = \frac{1}{3} \approx 0.33
\end{split}
\end{equation*} 
where FPR stands for \textit{False Positive Rate}.

\hfill

\noindent \textbf{(c)} The accuracy ($A$) is given by
\begin{equation*}
\begin{split}
A = \frac{TP + TN}{P + N}
\end{split}
\end{equation*} 
where TP is the number of true positives, TN is the number of true negatives, P is the total of positives, and N is 
the total of negatives. The accuracy for Table 4 then is
\begin{equation*}
\begin{split}
A = \frac{56 + 41}{58 + 42} = \frac{97}{100} = 0.97 
\end{split}
\end{equation*} 

\section*{Question 8} \textbf{Doubt: The expected cost of predicting negative should be: (cost of false negative * prob of false negative) + 
 (cost of true negative * prob of true negative). Given that the cost of a true negative is zero, the expected cost of predicting negative is $a \times P(C = - | x)$, no? }

\noindent \textbf{(a)} According to the data in the question, \textit{spam} is the positive class (+) and \textit{non-spam} is the negative class. Given that a false negative 
prediction costs 5 units, and that a false positive prediction costs 2, we have that
\begin{equation*}
\begin{split}
&E[predicting\mbox{ }spam] = 2 \times P(non-spam|x1, x2)\\
&E[predicting\mbox{ }spam] = 2 \times (1 - P(spam|x1, x2))\\
&E[predicting\mbox{ }spam] = 2 \times (1 - \frac{1}{e^{-(3x_2 - 2x_1 + 1)}})\\
&E[predicting\mbox{ }spam] = 2 \times (1 - \frac{1}{e^{-(3 \times 2 - 2 \times 3 + 1)}})\\
&E[predicting\mbox{ }spam] = 2 \times (1 - \frac{1}{e})\\
&E[predicting\mbox{ }spam] = 2 \times (1 - \frac{1}{e})\\
&E[predicting\mbox{ }spam] \approx 2 \times 0.63\\
&E[predicting\mbox{ }spam] \approx 1.26
\end{split}
\end{equation*} 
and
\begin{equation*}
\begin{split}
&E[predicting\mbox{ }non-spam] = 5 \times P(spam|x1, x2)\\
&E[predicting\mbox{ }non-spam] = 5 \times \frac{1}{e^{-(3x_2 - 2x_1 + 1)}}\\
&E[predicting\mbox{ }non-spam] = 5 \times \frac{1}{e^{-(3 \times 2 - 2 \times 3 + 1)}}\\
&E[predicting\mbox{ }non-spam] = 5 \times \frac{1}{e}\\
&E[predicting\mbox{ }non-spam] \approx 1.84
\end{split}
\end{equation*} 
That is, classifying as \textit{spam} has smaller expected risk.

\section*{Question 9} \textbf{TODO}

\section*{Question 10} \textbf{(a)} Given the dataset, we have the following positive examples:
\begin{gather*}
\begin{split}
&\textbf{x}_+^1 = 
 \begin{bmatrix}
   2.7 \\
   4.8 \\
 \end{bmatrix}\\
&\textbf{x}_+^2 = 
 \begin{bmatrix}
   3.2 \\
   5.1 \\
\end{bmatrix}\\
&\textbf{x}_+^3 = 
 \begin{bmatrix}
   -0.4 \\
   -0.3\\
 \end{bmatrix}
\end{split}
\end{gather*}

where the first and second rows correspond, respectively, to values for attributes $x1$ and $x2$. 
Consequently,  
\begin{gather*}
\begin{split}
&\hat{\mu}_+ = 
 \frac{ \begin{bmatrix}
   2.7 \\
   4.8\\ 
 \end{bmatrix} +  \begin{bmatrix}
   3.2 \\
   5.1\\ 
 \end{bmatrix} +  \begin{bmatrix}
   -0.4 \\
   -0.3 \\
 \end{bmatrix}}{3}\\
&\hat{\mu}_+ = 
 \frac{\begin{bmatrix}
   2.7 + 3.2 - 0.4\\
   4.8 + 5.1 - 0.3\\ 
   \end{bmatrix}}{3}\\
&\hat{\mu}_+ = 
 \frac{\begin{bmatrix}
   5.5\\
   9.6 
   \end{bmatrix}}{3}\\
&\hat{\mu}_+ \approx 
\begin{bmatrix}
   1.8\\
   3.2 
   \end{bmatrix}
\end{split}
\end{gather*}
where $\hat{\mu}_+[1][1] = 1.8$ and $\hat{\mu}_+[2][1] = 3.2$ are the estimated means for attributes $x1$ and $x2$ respectively, when labels are positive.

\noindent Analogously, the negative examples are
\begin{gather*}
\begin{split}
&\textbf{x}_-^1 = 
 \begin{bmatrix}
   0.6 \\
   0.5 \\
 \end{bmatrix}\\
&\textbf{x}_-^2 = 
 \begin{bmatrix}
   1.8 \\
   2.8 \\
\end{bmatrix}\\
&\textbf{x}_-^3 = 
 \begin{bmatrix}
   2.1 \\
   4.3\\
 \end{bmatrix}
\end{split}
\end{gather*}
Consequently,
\begin{gather*}
\begin{split}
&\hat{\mu}_- = 
 \frac{ \begin{bmatrix}
   0.6 \\
   0.5\\ 
 \end{bmatrix} +  \begin{bmatrix}
   1.8 \\
   2.8\\ 
 \end{bmatrix} +  \begin{bmatrix}
   2.1\\
   4.3\\
 \end{bmatrix}}{3}\\
&\hat{\mu}_- = 
 \frac{\begin{bmatrix}
   0.6 + 1.8 + 2.1\\
   0.5 + 2.8 + 4.3\\ 
   \end{bmatrix}}{3}\\
&\hat{\mu}_- = 
 \frac{\begin{bmatrix}
   4.5\\
   7.6 
   \end{bmatrix}}{3}\\
&\hat{\mu}_- \approx 
\begin{bmatrix}
   1.5\\
   2.5 
   \end{bmatrix}
\end{split}
\end{gather*}
where $\hat{\mu}_+[1][1] = 1.5$ and $\hat{\mu}_+[2][1] = 2.5$ are the estimated means for attributes $x1$ and $x2$ respectively, when labels are negative.

\noindent Given the positive examples and $\hat{\mu}_+$, we compute
\begin{gather*}
\begin{split}
&\textbf{x}_+^1 - \hat{\mu}_+ = 
\begin{bmatrix}
   2.7\\
   4.8 
\end{bmatrix} - 
\begin{bmatrix}
   1.8\\
   3.2 
\end{bmatrix} = 
\begin{bmatrix}
   0.9\\
   1.6 
\end{bmatrix}\\
&\textbf{x}_+^2 - \hat{\mu}_+ = 
\begin{bmatrix}
   3.2\\
   5.1 
\end{bmatrix} - 
\begin{bmatrix}
   1.8\\
   3.2 
\end{bmatrix} = 
\begin{bmatrix}
   1.4\\
   1.9 
\end{bmatrix}\\
&\textbf{x}_+^3 - \hat{\mu}_+ = 
\begin{bmatrix}
   -0.4\\
   -0.3 
\end{bmatrix} - 
\begin{bmatrix}
   1.8\\
   3.2 
\end{bmatrix} = 
\begin{bmatrix}
   -2.2\\
   -3.5 
\end{bmatrix}\\
\end{split}
\end{gather*}
Thus
\begin{gather*}
\begin{split}
&\hat{\Sigma}_+ = 
\frac{\begin{bmatrix}
   0.9\\
   1.6 
\end{bmatrix} \times \begin{bmatrix}
   0.9 & 1.6\\
\end{bmatrix} + 
\begin{bmatrix}
   1.4\\
   1.9 
\end{bmatrix} \times \begin{bmatrix}
   1.4 & 1.9\\
\end{bmatrix} +
\begin{bmatrix}
   -2.2\\
   -3.5 
\end{bmatrix} \times \begin{bmatrix}
   -2.2 & -3.5\\
\end{bmatrix}}{3}\\ 
&\hat{\Sigma}_+ = 
\frac{\begin{bmatrix}
   0.81 & 1.44\\
   1.44 & 2.56 
\end{bmatrix} + 
\begin{bmatrix}
   1.96 & 2.66\\
   2.66 & 3.61 
\end{bmatrix} +
\begin{bmatrix}
   4.84 & 7.7\\
   7.7 & 12.25 
\end{bmatrix}
 }{3}\\
&\hat{\Sigma}_+ = 
\frac{\begin{bmatrix}
   7.61 & 11.8\\
   11.8 & 18.42 
\end{bmatrix}}{3}\\
&\hat{\Sigma}_+ = 
\begin{bmatrix}
   2.54 & 3.93\\
   3.93 & 6.14 
\end{bmatrix}\\
\end{split}
\end{gather*}

\noindent Analogously, given the negative examples and $\hat{\mu}_-$, we compute
\begin{gather*}
\begin{split}
&\textbf{x}_-^1 - \hat{\mu}_- = 
\begin{bmatrix}
   0.6\\
   0.5 
\end{bmatrix} - 
\begin{bmatrix}
   1.5\\
   2.5 
\end{bmatrix} = 
\begin{bmatrix}
   -0.9\\
   -2.0 
\end{bmatrix}\\
&\textbf{x}_-^2 - \hat{\mu}_- = 
\begin{bmatrix}
   1.8\\
   2.8 
\end{bmatrix} - 
\begin{bmatrix}
   1.5\\
   2.5 
\end{bmatrix} = 
\begin{bmatrix}
   0.3\\
   0.3 
\end{bmatrix}\\
&\textbf{x}_-^3 - \hat{\mu}_- = 
\begin{bmatrix}
   2.1\\
   4.3 
\end{bmatrix} - 
\begin{bmatrix}
   1.5\\
   2.5 
\end{bmatrix} = 
\begin{bmatrix}
   0.6\\
   1.8 
\end{bmatrix}\\
\end{split}
\end{gather*}
Thus
\begin{gather*}
\begin{split}
&\hat{\Sigma}_- = 
\frac{\begin{bmatrix}
   -0.9\\
   -2.0 
\end{bmatrix} \times \begin{bmatrix}
   -0.9 & -2.0\\
\end{bmatrix} + 
\begin{bmatrix}
   0.3\\
   0.3 
\end{bmatrix} \times \begin{bmatrix}
   0.3 & 0.3\\
\end{bmatrix} +
\begin{bmatrix}
   0.6\\
   1.8 
\end{bmatrix} \times \begin{bmatrix}
   0.6 & 1.8\\
\end{bmatrix}}{3}\\ 
&\hat{\Sigma}_- = 
\frac{\begin{bmatrix}
   0.81 & 1.8\\
   1.8 & 4.0 
\end{bmatrix} + 
\begin{bmatrix}
   0.09 & 0.09\\
   0.09 & 0.09 
\end{bmatrix} +
\begin{bmatrix}
   0.36 & 1.08\\
   1.08 & 3.24 
\end{bmatrix}
 }{3}\\
&\hat{\Sigma}_- = 
\frac{\begin{bmatrix}
   1.26 & 2.97\\
   2.97 & 7.33 
\end{bmatrix}}{3}\\
&\hat{\Sigma}_- = 
\begin{bmatrix}
   0.42 & 0.99\\
   0.99 & 2.44 
\end{bmatrix}\\
\end{split}
\end{gather*}

\end{document}

