%This is my super simple Real Analysis Homework template

\documentclass[leqno]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[]{amsthm} %lets us use \begin{proof}
\usepackage{amsmath}
\usepackage[]{amssymb} %gives us the character \varnothing

\title{Homework 4 -- Part I}
\author{Aline Bessa and Rao Li}
\date\today
%This information doesn't actually show up on your document unless you use the maketitle command below

\begin{document}
\maketitle %This command prints the title based on information entered above

%Section and subsection automatically number unless you put the asterisk next to them.
\section*{Question 1} \textbf{(a)} By using scipy's package \textit{optimize}, we found that, at the local
minimum, $x = 1.93496079283$ and, at the global minimum, $x = -0.27922951$.

\hfill

\noindent \textbf{(b)} When we run gradient descent for 6 iterations starting with $x = -1$, the values we get are:
\begin{itemize}
\item $1^{st}$ iteration --- before, $x = -1$ and $f(x) = 22$; after, $x = -0.927$ and $f(x) = 17.085374066$
\item $2^{nd}$ iteration --- before, $x = -0.927$ and $f(x) = 17.085374066$; after, $x = -0.865190627272$ and $f(x) = 13.5381732809$
\item $3^{rd}$ iteration --- before, $x = -0.865190627272$ and $f(x) = 13.5381732809$; after, $x = -0.812109184654$ and $f(x) = 10.9075662033$
\item $4^{th}$ iteration --- before, $x = -0.812109184654$ and $f(x) = 10.9075662033$; after, $x = -0.765994669596$ and  $f(x) = 8.9130572554$
\item $5^{th}$ iteration --- before, $x = -0.765994669596$ and  $f(x) = 8.9130572554$; after, $x = -0.725548002932$ and $f(x) = 7.37275424133$
\item $6^{th}$ iteration --- before, $x = -0.725548002932$ and $f(x) = 7.37275424133$; after, $x = -0.689785964595$ and $f(x) = 6.16460298372$  
\end{itemize}  
The 6 last values we obtain when we run gradient descent with 1200 iterations are all the same:
$x = -0.279229509019$ and $f(x) = 0.416251130503$. Value $x$ has thus converged --- in this case, to the global minimum.

\hfill

\noindent \textbf{(c)} When we run gradient descent for 6 iterations starting with $x = 3$, the values we get are:
\begin{itemize}
\item $1^{st}$ iteration --- before, $x = 3$ and $f(x) = 50$; after, $x = 2.897$ and $f(x) = 40.3314182125$
\item $2^{nd}$ iteration --- before, $x = 2.897$ and $f(x) = 40.3314182125$; after, $x = 2.81191919263$ and $f(x) = 33.6675690732$
\item $3^{rd}$ iteration --- before, $x = 2.81191919263$ and $f(x) = 33.6675690732$; after, $x = 2.74013044847$ and $f(x) = 28.8855040619$
\item $4^{th}$ iteration --- before, $x = 2.74013044847$ and $f(x) = 28.8855040619$; after, $x = 2.67854155269$ and $f(x) = 25.3431652813$
\item $5^{th}$ iteration --- before, $x = 2.67854155269$ and $f(x) = 25.3431652813$; after, $x = 2.62499117847$ and $f(x) = 22.6509518911$
\item $6^{th}$ iteration --- before, $x = 2.62499117847$ and $f(x) = 22.6509518911$; after, $x = 2.57791408117$ and $f(x) = 20.5609965832$  
\end{itemize}  
The 6 last values we obtain when we run gradient descent with 1200 iterations are all the same:
$x = 1.93496079203$ and $f(x) = 9.93722775631$. Value $x$ has thus converged to a local minimum.

\hfill

\noindent \textbf{(d)} When we run gradient descent for 6 iterations starting with $x = -1$, but with $\eta = 0.01$ instead, the values we get are:
\begin{itemize}
\item $1^{st}$ iteration --- before, $x = -1$ and $f(x) = 22$; after, $x = -0.27$ and $f(x) = 0.41840264$
\item $2^{nd}$ iteration --- before, $x = -0.27$ and $f(x) = 0.41840264$; after, $x = -0.27464572$ and $f(x) = 0.416783708548$
\item $3^{rd}$ iteration --- before, $x = -0.27464572$ and $f(x) = 0.416783708548$; after, $x = -0.276965383222$ and $f(x) = 0.416381299356$
\item $4^{th}$ iteration --- before, $x = -0.276965383222$ and $f(x) = 0.416381299356$; after, $x = -0.27811422383$ and $f(x) = 0.416282743082$
\item $5^{th}$ iteration --- before, $x = -0.27811422383$ and $f(x) = 0.416282743082$; after, $x = -0.278680878569$ and $f(x) = 0.416258783583$
\item $6^{th}$ iteration --- before, $x = -0.278680878569$ and $f(x) = 0.416258783583$; after, $x = -0.278959808526$ and $f(x) = 0.416252980336$  
\end{itemize}  
The 6 last values we obtain when we run gradient descent with 1200 iterations are all the same:
$x = -0.279229509019$ and $f(x) = 0.416251130503$. Value $x$ has thus converged --- in this case, to the global minimum.
With $\eta = 0.01$, the optimization converged much faster than with $\eta = 0.001$, and to the same minimum.

\hfill

\noindent \textbf{(e)} The learning rate $\eta = 0.1$ is too large, and the optimization jumped over the minimum it was attempting to reach (overshooting). To
illustrate, these are the results we got for the first iterations:
\begin{itemize}
\item $1^{st}$ iteration --- before, $x = -1$ and $f(x) = 22$; after, $x = 6.3$ and $f(x) = 3052.0694$
\item $2^{nd}$ iteration --- before, $x = 6.3$ and $f(x) = 3052.0694$; after, $x = -230.0302$ and $f(x) = 11382676907.5$
\item $3^{rd}$ iteration --- before, $x = -230.0302$ and $f(x) = 11382676907.5$; after, $x = 19713256.9453$ and $f(x) = 6.04078583818e+29$
\end{itemize}  

\hfill

\section*{Question 2} Given the following parameters for the input layer:
\begin{gather*}
\begin{split}
&W^{(1)} =
  \begin{bmatrix}
  W_{11}^{(1)} & W_{12}^{(1)}\\
  W_{21}^{(1)} & W_{22}^{(1)}\\ 
  \end{bmatrix}
  =
  \begin{bmatrix}
  1 & 2\\
  3 & 4\\ 
  \end{bmatrix}
\\
&b^{(1)} =
   \begin{bmatrix}
  b_1^{(1)} \\
  b_2^{(1)}\\ 
  \end{bmatrix}
  =
  \begin{bmatrix}
  1 \\
  -1\\ 
  \end{bmatrix}
\end{split}
\end{gather*}
We start by computing $z^{(2)}$ and $a^{(2)}$ for training example 
\begin{gather*}
\begin{split}
&x_1 =
  \begin{bmatrix}
  1\\
  0\\ 
  \end{bmatrix}
\end{split}
\end{gather*}
That is: 
\begin{gather*}
\begin{split}
&z^{(2)} = W^{(1)}x_1 + b^{(1)} =
  \begin{bmatrix}
  1 & 2\\
  3 & 4\\ 
  \end{bmatrix}
  \begin{bmatrix}
  1\\
  0\\ 
  \end{bmatrix}
  +
 \begin{bmatrix}
  1\\
  -1\\ 
  \end{bmatrix}
\\
&z^{(2)} = 
\begin{bmatrix}
  z_1^{(2)}\\
  z_2^{(2)}\\ 
  \end{bmatrix}
  =
  \begin{bmatrix}
  1*1 + 2*0\\
  3*1 + 4*0\\ 
  \end{bmatrix}
  +
 \begin{bmatrix}
  1\\
  -1\\ 
  \end{bmatrix}
=
 \begin{bmatrix}
  2\\
  2\\ 
  \end{bmatrix}
\\
&a^{(2)} = f(z^{(2)}) = \frac{1}{1 + e^{-z{(2)}}} =
\begin{bmatrix}
   a_1^{(2)}\\
   a_2^{(2)}\\ 
  \end{bmatrix}
 =
 \begin{bmatrix}
   0.881\\
   0.881\\ 
  \end{bmatrix}
\\
\end{split}
\end{gather*}
Given the following parameters for the hidden layer:
\begin{gather*}
\begin{split}
&W^{(2)} =
  \begin{bmatrix}
  W_{11}^{(2)} & W_{12}^{(2)}\\
  \end{bmatrix}
  =
  \begin{bmatrix}
  1 & 2\\
  \end{bmatrix}
\\
&b^{(2)} =
   \begin{bmatrix}
  b_1^{(2)} \\
  \end{bmatrix}
  =
  \begin{bmatrix}
  1 \\
  \end{bmatrix}
\end{split}
\end{gather*}
We can compute $z^{(3)}$ and $a^{(3)}$:
\begin{gather*}
\begin{split}
&z^{(3)} = W^{(2)}a^{(2)} + b^{(2)} =
 \begin{bmatrix}
  z_1^{(3)}\\
 \end{bmatrix}
 =
 \begin{bmatrix}
   1 & 2\\
  \end{bmatrix}
 \begin{bmatrix}
   0.881\\
   0.881\\ 
  \end{bmatrix} 
  +
   \begin{bmatrix}
   1\\
  \end{bmatrix}
  =
 \begin{bmatrix}
   3.642\\ 
  \end{bmatrix}
\\
&a^{(3)} = f(z^{(3)}) = \frac{1}{1 + e^{-z_1{(3)}}} =
 \begin{bmatrix}
   a_1^{(3)}\\ 
  \end{bmatrix}
 =
 \begin{bmatrix}
   0.974\\ 
  \end{bmatrix}
\end{split}
\end{gather*}
Now we can backpropagate the errors for example $x_1$, starting with the calculation of partial derivatives:
\textbf{EXPLAIN NOTATION SHORTCUT FOR F APPLIED FOR SINGLE ELEMENT. also: always use same number of floating points}
\begin{gather*}
\begin{split}
&\delta_1^{(3)} = \frac{\partial J}{\partial z^{(3)}} = -(y - f(z_1^{(3)}))f'(z_1^{(3)}) \\ 
&\delta_1^{(3)} =  -(y - f(z_1^{(3)}))f(z_1^{(3)})(1 - f(z_1^{(3)}))\\
&\delta_1^{(3)} = -(1 - 0.974)*0.974*(1 - 0.974) = -6.584*10^{-4}\\
&\delta_1^{(2)} = \delta_1^{(3)}W_{11}^{(2)}f'(z_1^{(2)}) = \delta_1^{(3)}W_{11}^{(2)}f(z_1^{(2)})(1 - f(z_1^{(2)}))\\
&\delta_1^{(2)} = -6.584*10^{-4}*1*0.881*(1 - 0.881) = -6.903*10^{-5}\\
&\delta_2^{(2)} = \delta_1^{(3)}W_{12}^{(2)}f'(z_2^{(2)}) = \delta_1^{(3)}W_{12}^{(2)}f(z_2^{(2)})(1 - f(z_2^{(2)}))\\
&\delta_2^{(2)} = -6.584*10^{-4}*2*0.881*(1 - 0.881) = -1.381*10^{-4}\\ 
&\frac{\partial J}{\partial W^{(1)}} = 
 \begin{bmatrix}
   \frac{\partial J}{\partial W_{11}^{(1)}} & \frac{\partial J}{\partial W_{12}^{(1)}}\\ 
  \frac{\partial J}{\partial W_{21}^{(1)}} & \frac{\partial J}{\partial W_{22}^{(1)}}\\
  \end{bmatrix}
   =
 \begin{bmatrix}
   a_1^{(1)}\delta_1^{(2)} & a_2^{(1)}\delta_1^{(2)}\\ 
   a_1^{(1)}\delta_2^{(2)} & a_2^{(1)}\delta_2^{(2)}\\
  \end{bmatrix}\\
& \frac{\partial J}{\partial W^{(1)}} =
  \begin{bmatrix}
   1*-6.903*10^{-5} & 0*-6.903*10^{-5}\\ 
   1*-1.381*10^{-4} & 0*-1.381*10^{-4}\\
  \end{bmatrix}
 =
  \begin{bmatrix}
   -6.903*10^{-5} & 0\\ 
   -1.381*10^{-4} & 0\\
  \end{bmatrix}
\\
&\frac{\partial J}{\partial b^{(1)}} = 
 \begin{bmatrix}
   \frac{\partial J}{\partial b_1^{(1)}}\\ 
  \frac{\partial J}{\partial b_2^{(1)}}\\
  \end{bmatrix}
  =
 \begin{bmatrix}
   \delta_1^{(2)}\\ 
   \delta_2^{(2)}\\
  \end{bmatrix}
  =
  \begin{bmatrix}
   -6.903*10^{-5}\\ 
   -1.381*10^{-4}\\
  \end{bmatrix}
\\
&\frac{\partial J}{\partial W^{(2)}} = 
 \begin{bmatrix}
   \frac{\partial J}{\partial W_{11}^{(2)}} & \frac{\partial J}{\partial W_{12}^{(2)}}\\ 
  \end{bmatrix}
  =
 \begin{bmatrix}
   a_1^{(2)}\delta_1^{(3)} & a_2^{(2)}\delta_1^{(3)}\\ 
  \end{bmatrix}
\\
&\frac{\partial J}{\partial W^{(2)}}   =
  \begin{bmatrix}
   0.881*-6.584*10^{-4} & 0.881*-6.584*10^{-4}\\ 
  \end{bmatrix}
  =
  \begin{bmatrix}
   -5.801*10^{-4} & -5.801*10^{-4}\\
  \end{bmatrix}
\\
&\frac{\partial J}{\partial b^{(2)}} = [\delta_1^{(3)}] = [-6.584*10^{-4}]\\
\end{split}
\end{gather*}
\textbf{Where do values $a^{(1)}$ above come from?}
As for example 
\begin{gather*}
\begin{split}
&x_2 =
  \begin{bmatrix}
  0\\
  1\\ 
  \end{bmatrix}
\end{split}
\end{gather*}
We have: 
\begin{gather*}
\begin{split}
&z^{(2)} = W^{(1)}x_1 + b^{(1)} =
  \begin{bmatrix}
  1 & 2\\
  3 & 4\\ 
  \end{bmatrix}
  \begin{bmatrix}
  0\\
  1\\ 
  \end{bmatrix}
  +
 \begin{bmatrix}
  1\\
  -1\\ 
  \end{bmatrix}
\\
&z^{(2)} = 
  \begin{bmatrix}
  1*0 + 2*1\\
  3*0 + 4*1\\ 
  \end{bmatrix}
  +
 \begin{bmatrix}
  1\\
  -1\\ 
  \end{bmatrix}
=
 \begin{bmatrix}
  z_1^{(2)}\\
  z_2^{(2)}\\ 
  \end{bmatrix}
=
 \begin{bmatrix}
  3\\
  3\\ 
  \end{bmatrix}
\\
&a^{(2)} = f(z^{(2)}) = \frac{1}{1 + e^{-z{(2)}}} =
 \begin{bmatrix}
   a_1^{(2)}\\
   a_2^{(2)}\\ 
  \end{bmatrix}
=
 \begin{bmatrix}
   0.953\\
   0.953\\ 
  \end{bmatrix}
\\
&z^{(3)} = W^{(2)}a_2^{(2)} + b^{(2)} =
 \begin{bmatrix}
   1 & 2\\
  \end{bmatrix}
 \begin{bmatrix}
   0.953\\
   0.953\\ 
  \end{bmatrix} 
  +
   \begin{bmatrix}
   1\\
  \end{bmatrix}
  =
 \begin{bmatrix}
   3.858\\ 
  \end{bmatrix}
\\
&a^{(3)} = f(z^{(3)}) = \frac{1}{1 + e^{-z{(3)}}} =
 \begin{bmatrix}
   0.979\\ 
  \end{bmatrix}
\end{split}
\end{gather*}
Now, let's compute partial derivatives with respect to example $x_2$:
\textbf{Talk to Rao about the mix between matrices and scalars in notation...}
\begin{gather*}
\begin{split}
&\delta_1^{(3)} = -(y - f(z_1{(3)}))f'(z_1^{(3)}) = -(y - f(z_1^{(3)}))f(z_1^{(3)})(1 - f(z_1^{(3)}))\\
&\delta_1^{(3)} = -(0 - 0.979)0.979(1 - 0.979) = 0.020\\
&\delta_1^{(2)} = \delta_1^{(3)}W_{11}^{(2)}f'(z_1^{(2)}) = \delta_1^{(3)}W_{11}^{(2)}f(z_1^{(2)})(1 - f(z_1^{(2)}))\\
&\delta_1^{(2)} = 0.020*1*0.953*(1 - 0.953) = 8.958*10^{-4}\\
&\delta_2^{(2)} = \delta_1^{(3)}W_{12}^{(2)}f'(z_2^{(2)}) = \delta_1^{(3)}W_{12}^{(2)}f(z_2^{(2)})(1 - f(z_2^{(2)}))\\
&\delta_2^{(2)} = 0.020 * 2 * 0.953*(1- 0.953) = 1.792*10^{-3}\\
&\frac{\partial J}{\partial W^{(1)}} = 
 \begin{bmatrix}
   \frac{\partial J}{\partial W_{11}^{(1)}} & \frac{\partial J}{\partial W_{12}^{(1)}}\\ 
  \frac{\partial J}{\partial W_{21}^{(1)}} & \frac{\partial J}{\partial W_{22}^{(1)}}\\
  \end{bmatrix}
   =
 \begin{bmatrix}
   a_1^{(1)}\delta_1^{(2)} & a_2^{(1)}\delta_1^{(2)}\\ 
   a_1^{(1)}\delta_2^{(2)} & a_2^{(1)}\delta_2^{(2)}\\
  \end{bmatrix}\\
& \frac{\partial J}{\partial W^{(1)}} =
  \begin{bmatrix}
   0*8.958*10^{-4} & 1*8.958*10^{-4}\\ 
   0*1.792*10^{-3} & 1*1.792*10^{-3}\\
  \end{bmatrix}
 =
  \begin{bmatrix}
   0 & 8.958*10^{-4}\\ 
   0 & 1.792*10^{-3}\\
  \end{bmatrix}
\\
&\frac{\partial J}{\partial b^{(1)}} = 
 \begin{bmatrix}
   \frac{\partial J}{\partial b_1^{(1)}}\\ 
  \frac{\partial J}{\partial b_2^{(1)}}\\
  \end{bmatrix}
  =
 \begin{bmatrix}
   \delta_1^{(2)}\\ 
   \delta_2^{(2)}\\
  \end{bmatrix}
  =
  \begin{bmatrix}
   8.958*10^{-4}\\ 
   1.792*10^{-3}\\
  \end{bmatrix}
\\
&\frac{\partial J}{\partial W^{(2)}} = 
 \begin{bmatrix}
   \frac{\partial J}{\partial W_{11}^{(2)}} & \frac{\partial J}{\partial W_{12}^{(2)}}\\ 
  \end{bmatrix}
  =
 \begin{bmatrix}
   a_1^{(2)}\delta_1^{(3)} & a_2^{(2)}\delta_1^{(3)}\\ 
  \end{bmatrix}
\\
&\frac{\partial J}{\partial W^{(2)}}   =
  \begin{bmatrix}
   0.953*0.020 & 0.953*0.020\\ 
  \end{bmatrix}
  =
  \begin{bmatrix}
   0.019 & 0.019\\
  \end{bmatrix}
\\
&\frac{\partial J}{\partial b^{(2)}} = [\delta_1^{(3)}] = [0.020]\\
\end{split}
\end{gather*}
With these values, we can compute one step of gradient descent for all parameters in all layers.
\begin{gather*}
\begin{split}
&\Delta W^{(1)} = \sum_{i = 1}^N \frac{\partial J}{\partial W^{(1)}} =
  \begin{bmatrix}
    -6.903*10^{-5} + 0 & 0 + 8.958*10^{-4}\\
   -1.381*10^{-4} + 0 & 0 + 1.792*10^{-3}\\
  \end{bmatrix}
=
  \begin{bmatrix}
  -6.903*10^{-5} & 8.958*10^{-4}\\
  -1.381*10^{-4} & 1.792*10^{-3}\\
  \end{bmatrix}
  \\
&W_{new}^{(1)} = W^{(1)} - \alpha\frac{1}{N}\Delta W^{(1)}\\
&W_{new}^{(1)} = 
\begin{bmatrix}
1 & 2\\
3 & 4\\
\end{bmatrix}
- 0.2 * \frac{1}{2} *
\begin{bmatrix}
-6.903*10^{-5} & 8.958*10^{-4}\\
-1.381*10^{-4} & 1.792*10^{-3}\\
\end{bmatrix}\\
&W_{new}^{(1)} = 
\begin{bmatrix}
1.000 & 1.999\\
3.000 & 3.999\\
\end{bmatrix}\\
&\Delta b^{(1)} = \sum_{i = 1}^N \frac{\partial J}{\partial b^{(1)}} =
\begin{bmatrix}
-6.903*10^{-5} + 8.958*10^{-4} \\
-1.381*10^{-4} + 1.792*10^{-3}\\
\end{bmatrix}\\
&\Delta b^{(1)} =
\begin{bmatrix}
8.958*10^{-4} \\
1.792*10^{-3}\\
\end{bmatrix}\\
&b_{new}^{(1)} = b^{(1)} - \alpha\frac{1}{N}\Delta b^{(1)} = 
\begin{bmatrix}
1\\
-1\\
\end{bmatrix} 
- 0.2*\frac{1}{2} *
\begin{bmatrix}
8.958*10^{-4} \\
1.792*10^{-3}\\
\end{bmatrix}\\
&b_{new}^{(1)} = 
\begin{bmatrix}
0.999 \\
-1.000\\
\end{bmatrix}\\
&\Delta W^{(2)} = \sum_{i = 1}^N \frac{\partial J}{\partial W^{(2)}} =
  \begin{bmatrix}
    -5.801*10^{-4} + 0.019 +  & -5.801*10^{-4} + 0.019\\
  \end{bmatrix}\\
&\Delta W^{(2)} =
  \begin{bmatrix}
    0.018 & 0.018\\
  \end{bmatrix}\\
&W_{new}^{(2)} = W^{(2)} - \alpha\frac{1}{N}\Delta W^{(2)}\\
&W_{new}^{(2)} = 
  \begin{bmatrix}
    1 & 2\\
  \end{bmatrix}
- 0.2*\frac{1}{2}*
  \begin{bmatrix}
    0.018 & 0.018\\
  \end{bmatrix} =
  \begin{bmatrix}
    0.998 & 1.998\\
  \end{bmatrix}\\
&\Delta b^{(2)} = \sum_{i = 1}^N \frac{\partial J}{\partial b^{(2)}} =
\begin{bmatrix}
-6.584*10^{-4} + 0.020\\
\end{bmatrix} =
\begin{bmatrix}
0.0193416\\
\end{bmatrix}\\
&b_{new}^{(2)} = b^{(2)} - \alpha\frac{1}{N}\Delta b^{(2)} = 
\begin{bmatrix}
1\\
\end{bmatrix} - 0.2*\frac{1}{2}*
\begin{bmatrix}
0.0193416\\
\end{bmatrix}
=
\begin{bmatrix}
 0.998\\
\end{bmatrix}\\
\end{split}
\end{gather*}

\hfill
\section*{Question 3} \textbf{(a)} NeuralNetRK is the only network that can output negative values because scores $z$ can be negative. The other 
networks output probabilities, which are naturally in the interval $[0,1]$.

\hfill

\noindent \textbf{(b)} The outputs $y_1, \ldots, y_K$ of NeuralNetCK sum to 1 because of the way the activation functions for the output layer are defined. 

\hfill

\noindent \textbf{(c)} I would use NeuralNetCK with $K = 3$ classes. Given an example $x$, the outputs of this network correspond to the probabilities of $x$ belonging to 
 each one of the $K$ classes. Consequently, if $x$ is an input image and the $K = 3$ classes correspond, respectively, to \textit{face}, \textit{cat}, and \textit{tree}, the 
outputs of NeuralNetCK will be the probabilities $p_1 = P[face|x]$, $p_2 = P[cat|x]$, and $p_3 = P[tree|x]$, i.e., the probabilities required in this question. 

\hfill

\noindent \textbf{(d)} In this case, different configurations can be used. A possibility is to have two NeuralNetCB networks. Given a document $x$, the first 
network outputs $p_1 = P[politics|x]$, and if $p_1 > 0.5$ the output of this network is 1, i.e., we predict that document $x$ is on politics (otherwise we predict it is not 
on this topic). Similarly, the 
second network outputs $p_2 = P[formal|x]$, and if $p_2 > 0.5$ the output of this network is 1, i.e., we predict that document $x$ is formal (otherwise we predict it is 
informal). A different strategy would be to use a single NeuralNetCK and have $K = 4$ classes for a document $x$: class 1 if $x$ is on politics AND is formal, 
class 2 if $x$ is on politics AND is informal, class 3 if $x$ is not on politics AND is formal, and class 4 if $x$ is not on politics AND is informal. The output of this 
network would be the probability of each class, and the prediction would correspond to the highest probability, tackling both the topic and the formality of a document at 
once. 

\hfill

\section*{Question 4} Waiting for Rao

\end{document}
